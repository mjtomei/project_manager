project:
  name: project-manager
  repo: https://github.com/mjtomei/project_manager.git
  base_branch: master
  backend: github
  active_pr: pr-024
  guide_deps_reviewed: true
  repo_id: 828c8d0a91034cd541f443dd0aee62961e2af75a
  last_pr_sync: '2026-02-17T23:04:09.603896+00:00'
plans:
- id: plan-001
  name: Import from existing repo
  file: plans/plan-001.md
  status: draft
- id: plan-002
  name: Multi-candidate test-driven code generation
  file: plans/plan-002.md
  status: draft
- id: plan-003
  name: Multiuser Support
  file: plans/plan-003.md
  status: draft
prs:
- id: pr-001
  plan: plan-001
  title: Add background cluster computation to plan import
  branch: pm/pr-001-add-background-cluster-computation-to-plan-import
  status: pending
  depends_on: []
  description: Run cluster extraction in parallel when `_run_plan_import` starts,
    so results are available without blocking the interactive flow
  agent_machine: null
  gh_pr: null
- id: pr-002
  plan: plan-001
  title: Include cluster suggestions in import prompt
  branch: pm/pr-002-include-cluster-suggestions-in-import-prompt
  status: pending
  depends_on:
  - pr-001
  description: Add cluster analysis results as a "Phase 1.5" in the import prompt
    - Claude receives clusters as a suggested starting point for PR decomposition,
    which can be used, modified, or ignored
  agent_machine: null
  gh_pr: null
- id: pr-003
  plan: plan-001
  title: Add cluster-first option to guide workflow
  branch: pm/pr-003-add-cluster-first-option-to-guide-workflow
  status: pending
  depends_on: []
  description: In the guide's "initialized" step, offer `pm cluster explore` as an
    alternative path alongside `pm plan add`, giving users a choice between manual
    and automated starting points
  agent_machine: null
  gh_pr: null
- id: pr-004
  plan: plan-001
  title: Smart repo-size detection with approach recommendation
  branch: pm/pr-004-smart-repo-size-detection-with-approach-recommenda
  status: pending
  depends_on:
  - pr-003
  description: Detect repo size (lines of code) and recommend manual vs cluster-based
    approach automatically - small repos get manual suggestion, large repos get cluster
    suggestion
  agent_machine: null
  gh_pr: null
- id: pr-005
  plan: plan-001
  title: Improve git remote detection in pm init
  branch: pm/pr-005-improve-git-remote-detection-in-pm-init
  status: merged
  depends_on: []
  description: pm init hardcodes 'origin' remote, but repos may have multiple remotes
    or use different naming (e.g., 'github', 'upstream'). Should detect the correct
    remote by checking if it exists/works, letting user choose when ambiguous, or
    preferring remotes matching the detected backend.
  agent_machine: spark-424d
  gh_pr: null
  workdir: null
- id: pr-006
  plan: plan-001
  title: Add automatic PR sync on TUI refresh and periodic background check
  branch: pm/pr-006-add-automatic-pr-sync-on-tui-refresh-and-periodic-
  status: merged
  depends_on: []
  description: 'Currently pm pr sync must be run manually. Add: (1) automatic sync
    when TUI refreshes (r key), (2) periodic background sync every 5 minutes in TUI,
    (3) sync before pr-related commands. Store last sync timestamp to avoid excessive
    API calls.'
  agent_machine: spark-424d
  gh_pr: null
  workdir: null
- id: pr-007
  plan: plan-001
  title: 'GitHub backend: Create draft PR at start, upgrade on done'
  branch: pm/pr-007-github-backend-create-draft-pr-at-start-upgrade-on
  status: merged
  depends_on: []
  description: 'Change GitHub backend flow: (1) pm pr start creates a draft PR immediately
    after first push, stores gh_pr number in project.yaml, (2) pm pr done upgrades
    draft to ready-for-review using ''gh pr ready'', (3) status becomes in_review
    only after upgrading. This gives visibility into WIP earlier and uses GitHub''s
    native draft PR workflow.'
  agent_machine: spark-424d
  gh_pr: null
  workdir: null
- id: pr-009
  plan: plan-001
  title: Auto-rebalance after pane launch
  branch: pm/pr-009-auto-rebalance-after-pane-launch
  status: merged
  depends_on: []
  description: After launching a new pane, the layout should automatically rebalance.
    Currently requires manual 'b' keypress to rebalance.
  agent_machine: spark-424d
  gh_pr: null
  workdir: null
- id: pr-010
  plan: plan-001
  title: Isolate workdir pm state from main repo
  branch: pm/pr-010-isolate-workdir-pm-state-from-main-repo
  status: merged
  depends_on: []
  description: 'Agents in workdirs can modify pm/project.yaml and push to GitHub,
    which then gets synced to the main TUI causing unexpected state changes (test
    PRs appearing, status changes, etc.).


    Fix options:

    1. Workdirs should not include pm/ directory at all - agents work on code only

    2. Or workdirs get a read-only copy of pm/ that isn''t pushed

    3. Or agents are instructed not to modify pm state except their own PR status

    4. Or git_ops.sync_state() should only pull pm/ changes from master, not from
    PR branches


    Recommended: Option 1 - exclude pm/ from workdir clones since agents only need
    the code, not the project management state.'
  agent_machine: spark-424d
  gh_pr: null
  workdir: null
- id: pr-011
  title: Fix plans pane w key launching wrong action
  branch: null
  status: merged
  depends_on: []
  description: 'In the plans pane, pressing w (breakdown) sometimes launches pm plan
    review instead of pm plan breakdown. The PlanAction("breakdown") message is posted
    correctly by plans_pane.py, but the app handler routes it to the wrong command.


    Reproduction steps:

    1. Open the TUI and press P to enter plans view

    2. Select a plan (e.g. plan-001)

    3. Press w to trigger breakdown

    4. Check pane registry: the new pane has role "plan-review" running "pm plan review
    plan-001" instead of role "plan-breakdown" running "pm plan breakdown plan-001"

    5. Also observed: first w press sometimes produces no pane at all (no log entry
    for "plan action: breakdown"), while a second w press creates the wrong pane type


    The mismatch may indicate the on_plan_action handler is receiving a different
    action string than expected, or there is a code path where the running TUI code
    diverges from the source on disk.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/32
  gh_pr_number: 32
  workdir: null
- id: pr-012
  plan: plan-002
  title: Local model runner with OpenAI-compatible API
  branch: pm/pr-012-local-model-runner-with-openai-compatible-api
  status: in_review
  depends_on: []
  description: 'Create a module that manages local LLM inference via the OpenAI-compatible
    API exposed by local serving backends (see `pm/plans/plan-002.md` for the broader
    multi-candidate generation strategy). Support three backends with automatic platform
    detection: llama.cpp server on macOS, sglang and vllm on Linux. All three expose
    the same OpenAI-compatible chat/completions endpoint, so the core runner uses
    a single HTTP client with backend-specific server management (health checks, model
    listing). Server URL is configured via `PM_BENCH_URL` environment variable with
    a sensible default per backend (e.g. `http://localhost:8080` for llama.cpp, `http://localhost:30000`
    for sglang). Support sending prompts to a configurable model, collecting responses,
    and running multiple generations with different temperatures in parallel. Track
    token counts and wall-clock time per request for downstream cost analysis. Include
    a CLI command (`pm bench models`) that detects the available backend, lists loaded
    models, and validates connectivity.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/33
  gh_pr_number: 33
  workdir: null
- id: pr-013
  plan: plan-002
  title: Aider polyglot exercise loader
  branch: pm/pr-013-aider-polyglot-exercise-loader
  status: in_review
  depends_on: []
  description: 'Load the Exercism exercises used by aider''s polyglot benchmark (https://github.com/Aider-AI/aider
    — see `benchmark/` directory for exercise structure). Each exercise has a problem
    description, starter code, and a reference test suite. Parse exercises into a
    structured format: {language, slug, description, starter_code, reference_tests}.
    Support filtering by language and exercise name. Clone or download the exercise
    set from aider''s benchmark repo and cache it locally under `~/.cache/pm-bench/exercises/`.
    Include a CLI command (`pm bench exercises`) that downloads/updates the exercise
    cache and lists available exercises with optional `--language` filter.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/34
  gh_pr_number: 34
  workdir: null
- id: pr-014
  plan: plan-002
  title: Test generation from problem descriptions
  branch: pm/pr-014-test-generation-from-problem-descriptions
  status: in_progress
  depends_on: []
  description: 'Given an exercise''s problem description and starter code (but NOT
    the reference tests), generate test cases using a local model. The core insight
    from the plan (`pm/plans/plan-002.md`): verification is easier than generation
    — a model that scores poorly on single-pass coding can generate useful tests that
    filter better solutions from multiple candidates. Use the problem description
    and function signatures to produce tests that verify correctness. Generate tests
    at multiple temperatures and with prompt variations to increase diversity. Deduplicate
    and validate generated tests (must parse, must be syntactically valid, must reference
    the correct function names). Return a merged test suite as a single source file
    string per language''s test convention.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/35
  gh_pr_number: 35
  workdir: null
- id: pr-015
  plan: plan-002
  title: Multi-candidate solution generation
  branch: pm/pr-015-multi-candidate-solution-generation
  status: in_progress
  depends_on: []
  description: Given an exercise's starter code, problem description, and a test suite
    (either generated or reference), produce N candidate solutions using a local model.
    Vary temperature (0.0 to 1.0) and prompt format (direct, chain-of-thought, example-driven).
    Multi-model support (querying different models on the same backend) is a stretch
    goal — the initial implementation should work well with a single model and temperature/prompt
    diversity. Each candidate is a complete file that should compile and pass the
    tests. Return candidates as a list of {code, temperature, prompt_variant, model}.
    See `pm/plans/plan-002.md` for how candidate diversity feeds into tournament selection.
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/36
  gh_pr_number: 36
  workdir: null
- id: pr-016
  plan: plan-002
  title: Test execution and candidate scoring
  branch: pm/pr-016-test-execution-and-candidate-scoring
  status: pending
  depends_on:
  - pr-013
  description: 'Run a test suite against a candidate solution in an isolated environment.
    Use temporary directories with copies of the exercise scaffold — each candidate
    gets its own temp directory, solution file is written in, and the language-appropriate
    test runner is invoked via subprocess with a timeout. Support the 6 polyglot languages
    with appropriate build/test commands: `pytest` for Python, `go test` for Go, `cargo
    test` for Rust, `npm test` / `jest` for JavaScript, `javac` + `junit` for Java,
    `cmake` + `ctest` or `g++` + run for C++. Return per-test pass/fail results and
    an overall score. Score candidates as (passing_tests / total_tests). Handle compilation
    failures, timeouts, and runtime errors gracefully.'
  agent_machine: null
  gh_pr: null
  gh_pr_number: null
- id: pr-017
  plan: plan-002
  title: Benchmark orchestrator with tournament selection
  branch: pm/pr-017-benchmark-orchestrator-with-tournament-selection
  status: in_progress
  depends_on: []
  description: 'Wire together the full pipeline from `pm/plans/plan-002.md`: for each
    exercise, (1) generate tests from the description, (2) generate N candidate solutions,
    (3) score each candidate against the generated tests, (4) pick the best candidate,
    (5) score the best candidate against the reference tests to get the final result.
    The pipeline embodies the plan''s core insight — separating verification (test
    generation) from generation (solution candidates) lets weaker models punch above
    their weight through selection pressure. Compare the tournament score against
    a single-pass baseline (N=1, reference tests only). Collect cost metrics from
    the runner (total tokens, wall-clock time, tokens per exercise) alongside scores.
    Report results as a terminal table showing per-exercise and aggregate scores,
    and save raw results to a JSON file. Add CLI command `pm bench run` with options
    for model, N candidates, languages, and exercise filter.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/37
  gh_pr_number: 37
  workdir: null
- id: pr-018
  plan: plan-002
  title: Baseline measurement and analysis
  branch: pm/pr-018-baseline-measurement-and-analysis
  status: pending
  depends_on:
  - pr-017
  description: 'Add a CLI command `pm bench analyze` that reads benchmark result JSON
    files produced by the orchestrator and generates comparison reports. Run the benchmark
    via `pm bench run` in single-pass mode (N=1, no test generation) to establish
    the baseline, then with tournament (N=8, N=16) and compare. Produce a report showing:
    baseline vs tournament scores per language, per exercise difficulty tier (easy/medium/hard,
    based on baseline pass rate — easy: >66% of models solve it, hard: <33%), and
    aggregate. Include cost analysis — total tokens generated, wall-clock time, and
    tokens-per-correct-exercise for each configuration, so the accuracy improvement
    can be weighed against the compute cost (e.g., "N=16 tournament scores 58% vs
    40% baseline but costs 20x the tokens"). Identify which exercises benefit most
    from multi-candidate generation (large delta) vs which are insensitive (model
    either always gets it or never does). Save analysis results to a JSON file for
    tracking over time.'
  agent_machine: null
  gh_pr: null
  gh_pr_number: null
- id: pr-019
  plan: plan-002
  title: Generated test quality analysis
  branch: pm/pr-019-generated-test-quality-analysis
  status: pending
  depends_on:
  - pr-017
  description: 'Add a CLI command `pm bench test-quality` that analyzes how well generated
    tests compare to reference tests. This is critical for validating the plan''s
    core assumption (`pm/plans/plan-002.md`) that verification is easier than generation
    — if generated tests are poor, tournament selection breaks down regardless of
    candidate quality. Reads benchmark result JSON from the orchestrator, which includes
    both generated and reference test outcomes per candidate. Measure: (1) coverage
    overlap — run each candidate against both generated and reference tests and compute
    rank correlation (do generated tests rank candidates in the same order as reference
    tests?), (2) false positives — identify generated tests that pass on candidates
    that fail all reference tests (using the failed candidates already present in
    benchmark results), (3) diversity — count distinct pass/fail signatures across
    generated tests (more signatures = more behavioral coverage). Cross-reference
    with benchmark results: do exercises where generated tests diverge most from reference
    tests also show the largest gap between tournament-selected and oracle-selected
    (best candidate scored against reference tests) scores?'
  agent_machine: null
  gh_pr: null
  gh_pr_number: null
- id: pr-020
  plan: null
  title: Fix TUI command deduplication for concurrent PR actions
  branch: pm/pr-020-fix-tui-command-deduplication-for-concurrent-pr-ac
  status: in_review
  depends_on: []
  description: 'The TUI allows the same action (e.g. pr start) to be triggered multiple
    times before the first invocation completes, and allows the same action on multiple
    PRs simultaneously, causing race conditions and buggy behavior. Add guards to
    prevent: (1) re-triggering an action while it is still running, (2) running conflicting
    actions on different PRs concurrently (e.g. two pr starts). Track in-flight actions
    and either queue or reject duplicates, showing a status message to the user.'
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/38
  gh_pr_number: 38
  workdir: null
- id: pr-021
  plan: null
  title: Fix tmux commands to target current grouped session instead of base
  branch: pm/pr-021-fix-tmux-commands-to-target-current-grouped-sessio
  status: merged
  depends_on: []
  description: Commands that switch windows or interact with the tmux session (e.g.
    automatic window switching) target the base session name instead of the current
    grouped session. Need a helper that detects the current grouped session and targets
    that, falling back to the base session when not inside tmux.
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/39
  gh_pr_number: 39
  workdir: null
- id: pr-022
  plan: plan-003
  title: Handle merging pm directory changes across clones
  branch: pm/pr-022-handle-merging-pm-directory-changes-across-clones
  status: merged
  depends_on: []
  description: When PRs are added in two separate clones/workdirs, the pm/project.yaml
    can diverge. Need a strategy for merging pm directory changes (e.g. new PRs added
    concurrently) — could be a git pull/merge of the pm directory, a TUI key to sync,
    or automatic conflict resolution.
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/40
  gh_pr_number: 40
  workdir: null
- id: pr-023
  plan: null
  title: Fix rebalance layout for grouped sessions with different terminal sizes
  branch: pm/pr-023-fix-rebalance-layout-for-grouped-sessions-with-dif
  status: merged
  depends_on: []
  description: When multiple terminals are attached via grouped sessions with different
    aspect ratios (e.g. portrait vs landscape), rebalance computes layout based on
    the shared window size (minimum of all clients) rather than the calling terminal's
    size. This means a portrait monitor doesn't get a horizontal split layout. Need
    to investigate tmux aggressive-resize or alternative approaches.
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/41
  gh_pr_number: 41
  workdir: null
- id: pr-024
  plan: plan-003
  title: Add multi-user support to sessions
  branch: pm/pr-024-add-multi-user-support-to-sessions
  status: in_review
  depends_on: []
  description: Add support for two users on the same computer accessing the same pm
    session simultaneously. This is done by creating a tmux session with a session
    file that is accessible to a given group or all users on the system optionally.
    The choice between groups or global accessibility is controlled by a session command
    flag. The default continues to be accessibility only to the current user. Other
    users can access the same session by passing the same flags as well as the directory
    in which the original user started the session. The code uses that directory to
    correctly identify what the session tag is.
  agent_machine: spark-424d
  gh_pr: https://github.com/mjtomei/project_manager/pull/52
  gh_pr_number: 52
  workdir: null
