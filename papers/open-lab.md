# Engineering management

We need a research lab where the lab itself is open source. Not just the
research outputs — the organizational structure, decision-making
processes, resource allocation, and governance are all visible,
forkable, and improvable by anyone.

Management becomes engineering. The mechanisms that detect problems,
recognize achievements, enforce accountability, and surface
contributions are automated, versioned, and auditable — not left to
the discretion of whoever holds a management title. Both the positive
reinforcement (recognition, credit, visibility) and the negative
reinforcement (integrity checks, conflict detection, staleness
warnings) are code in the repo. Anyone can read them, propose changes,
or argue they're wrong.

The lab is managed through changes to a public management project on
GitHub. Anyone can see how the lab operates, propose changes to its
structure, and fork the entire organization to create their own.

## The end of organizational opacity

Machine intelligence is making organizational opacity unsustainable.
AI systems already aggregate public filings, leaked documents, employment
records, financial data, and published research into coherent pictures
of what organizations are actually doing — regardless of what those
organizations say they're doing. Any institution that relies on opacity
for control is losing that control whether it acts or not.

Organizations that don't open up voluntarily will have their opacity
pierced by others. Companies are deploying blockchain-based supply
chain verification because they can't verify what opaque suppliers
claim about sourcing and labor practices — a market projected to grow
from $2.9 billion to $44.3 billion by 2034 [1]. The era of "trust us, it works" is ending across investment — venture
investors now conduct spot-checks by contacting claimed customers
directly and demanding bank statements behind reported revenue, because
they can no longer take institutional self-reporting at face value [2].
Self-insured employers are building internal claims analytics because
they can't trust insurer-reported numbers — medical billing errors
account for an estimated $68 billion in unnecessary healthcare costs
annually, and half of those who dispute denied claims get them
reversed, suggesting the denials were wrong in the first place [3].

This is parallel construction at organizational scale. When institutions
are opaque, anyone with resources builds their own apparatus to see
through them. But it's not only the powerful who gain this capability.
The same AI tools that let investors reconstruct an institution's
internals are available to employees, researchers, and the public.
An individual can now aggregate an organization's public filings,
job postings, Glassdoor reviews, published outputs, and financial
records into a picture that used to require a dedicated analyst team.
The opacity that once protected mismanagement from scrutiny above
also protected it from scrutiny below. Both protections are ending
at the same time.

The result is a world where the people running opaque organizations
are the last to know what everyone else — funders, competitors,
and their own staff — already sees. The choice for anyone running
an organization is not between transparency and privacy. It's between
transparency you control — structured, auditable, on your terms —
and transparency imposed on you by others reconstructing your
internals from the outside.

[1] Market.us, "Blockchain for Supply Chain Traceability Market
Size," 2024.
<https://market.us/report/blockchain-for-supply-chain-traceability-market/>

[2] Triunity Capital Ventures, "Due Diligence in 2025: What Investors
Really Want to Know About Your AI Startup."
<https://triunitycapitalventures.com/due-diligence-in-2025-what-investors-really-want-to-know-about-your-ai-startup/>

[3] AJMC, "Survey Exposes Pervasive Billing Errors, Aggressive
Tactics in US Health Insurance," 2024.
<https://www.ajmc.com/view/survey-exposes-pervasive-billing-errors-aggressive-tactics-in-us-health-insurance>

## The cost of opaque institutions

Excess management and bureaucracy cost the U.S. economy more than
$3 trillion per year — roughly 17% of GDP [4]. The Fortune 500 alone accounts for
$480 billion in back-office inefficiencies [5]. The average firm
spends between 1.3% and 3.3% of its total wage bill on regulatory
compliance, with aggregate compliance costs ranging from $103 billion
to $289 billion annually [6]. Non-administrative staff spend 6.4 hours per week — 16% of a
standard work week — complying with internal regulation, and the
cost of self-imposed internal rules exceeds the cost of government
regulation by more than two to one [7].

Nepotism and favoritism compound these costs. Nearly 75% of employees report having worked in a toxic workplace,
with poor leadership — including favoritism and lack of accountability
— cited as the top cause by 79% of them [8]. MIT Sloan found that toxic culture is 10 times more important than
compensation in predicting turnover, and that it cost U.S. employers
nearly $50 billion per year before the Great Resignation [9]. When
unqualified hires hold leadership roles through connections rather
than competence, the remaining staff pick up the slack, leading to
burnout and further turnover. The organizational knowledge needed to
actually run the institution is never written down — it lives in
the heads of whoever happens to be politically connected enough to
stay.

The same patterns appear in public institutions. Researchers receiving
federal grants spend 42% of their time on administrative tasks rather
than research — unchanged from 2005 to 2012 despite reform efforts,
representing billions in lost productivity across the $40 billion the
U.S. invests annually in university R&D [10]. Universities absorb
$6.8 billion per year in unreimbursed overhead costs [11]. Nepotism
in public institutions creates wasteful overstaffing where unneeded
positions are created to employ relatives — documented across
municipalities as a drag on economic development [12]. Perceived
nepotism is negatively associated with educational investment across
countries, as measured by PISA scores [13]. Funding decisions happen
behind closed doors. Credit assignment is political. Access to
resources depends on who you know, not what you contribute.

[4] HBR / Gary Hamel, "Excess Management Is Costing the U.S.
$3 Trillion Per Year," 2016.
<https://hbr.org/2016/09/excess-management-is-costing-the-us-3-trillion-per-year>

[5] SSO Network, "Fortune 500: $480 billion in back office
inefficiencies a year."
<https://www.ssonetwork.com/continuous-improvement-process-improvement/articles/480-billion-in-back-office-inefficiencies-how-to>

[6] NBER / Cato, "The Cost of Regulatory Compliance in the
United States."
<https://www.cato.org/research-briefs-economic-policy/cost-regulatory-compliance-united-states>

[7] Deloitte Australia, "Get out of your own way — Unleashing
productivity," cited in
<https://www.consultancy.uk/news/973/deloitte-compliance-costs-australian-economy-250-billion>

[8] iHire, "Toxic Workplace Trends Report," February 2025
(n=2,285 across 57 industries).
<https://www.ihire.com/about/press/ihire-toxic-workplace-trends-report-pr>

[9] Sull, Sull, et al., "Toxic Culture Is Driving the Great
Resignation," MIT Sloan Management Review, January 2022.
<https://sloanreview.mit.edu/article/toxic-culture-is-driving-the-great-resignation/>

[10] Federal Demonstration Partnership, "The FDP Faculty Burden
Survey," 2012. <https://pmc.ncbi.nlm.nih.gov/articles/PMC2887040/>

[11] Association of American Universities, "Frequently Asked Questions
about Facilities and Administrative Costs of Federally Sponsored
University Research."
<https://www.aau.edu/key-issues/frequently-asked-questions-about-facilities-and-administrative-costs>

[12] Tandfonline, "Nepotism, political competition and
overemployment," 2020.
<https://www.tandfonline.com/doi/full/10.1080/2474736X.2020.1781542>

[13] ScienceDirect, "Nepotism, human capital and economic
development," 2020.
<https://www.sciencedirect.com/science/article/abs/pii/S0167268120304431>

## Open-sourcing the institution

Most "open science" initiatives open-source the outputs (papers, data,
code) but keep the institution opaque. This lab open-sources the
institution itself: governance, resource flows, decision history,
credit assignment.

The structure is the product. If someone disagrees with how resources
are allocated, they can see the exact process, propose a change, or
fork the whole thing and run it differently.

This isn't all-or-nothing. An organization doesn't have to go from
political hell to fully open overnight. The move can be done gradually.
Start by making governance documents public. Then resource tracking.
Then decision rationale. Each step is independently useful and each
step makes the next one easier. The tools described here support any
point on that spectrum.

## Relationship to existing institutions

The lab is additive, not exclusionary. It doesn't replace universities,
funding agencies, corporate labs, or governments. It runs alongside
them.

Researchers can participate in the open lab while holding positions
at existing institutions. Grants can come from traditional funders
(NSF, NIH, DARPA, private foundations) and flow through traditional
fiscal sponsors or university overhead structures. The lab's
transparency is about what happens with the resources, not about
requiring that resources move through new channels.

As long as alternative power structures exist — and they will for a
long time — people will have to interface with governments through
existing institutions for things like taxes, employment law, visa
sponsorship, and public funding compliance. The lab's design
acknowledges this. A researcher receiving a stipend through the lab
still files taxes through whatever legal entity employs them. A grant
received by the lab still goes through a fiscal sponsor that handles
reporting to the funder. The open layer sits on top of these
structures, making the decisions and flows visible, without pretending
the underlying structures don't exist.

The goal is not to tear down existing institutions. It's to build
something better in the open, let people use both, and let the
better system win over time by being more transparent, more auditable,
and more aligned with how researchers actually want to work.

This also matters for investors and funders. Any organization that is
not AI-visible — where a machine intelligence can't read and verify
what's actually happening — is relatively much more risky to fund.
As this kind of transparency becomes possible, it will become a
standard expectation. Organizations that refuse to operate visibly
when the tools exist to do so are signaling that they have something
to hide. The open lab structure will become a baseline requirement
for startups and research groups seeking funding, not because anyone
mandates it but because the alternative — trusting opaque institutions
when transparent ones are available — becomes an obviously worse bet.

## Architecture

Higher-level trees (the lab itself, divisions within it) have two repos:
a project management repo (managed by `pm`) and an organization repo.
Individual research projects below them may only have a pm repo and
their code repo. The two-repo structure is for organizational layers
that need to track more than just a tech tree.

### The organization repo

A GitHub repo that is the public record of the organization. Public
here means at least to all members — the org can choose to make it
world-readable too. This is where things live that don't belong in
any individual project but need to be tracked: documents, small
scripts, spreadsheets, test programs, analyses, meeting notes,
governance decisions, and anything else that should be visible and
version-controlled but doesn't need wide release as a standalone
project.

It also includes pointers to all the other repos in the organization
— the pm repo, the individual project repos, external dependencies —
in `docs/` so it can act as a central dispatch. A new member can start
at the org repo and navigate to anything the organization is working
on.

Structure:

```
org-repo/
├── docs/                        # golden copy of project state
│   ├── governance/              # decision-making processes, roles
│   ├── research-agenda/         # what the lab works on and why
│   ├── resources/               # grant tracking, allocation records
│   └── onboarding/              # how to join, how things work
├── members/
│   ├── alice/                   # Alice's working directory
│   │   ├── notes/               # her meeting notes, scratch work
│   │   ├── scripts/             # one-off analysis scripts
│   │   └── proposals/           # drafts she's working on
│   ├── bob/
│   │   ├── notes/
│   │   ├── data/                # small datasets, spreadsheets
│   │   └── experiments/         # test programs, prototypes
│   └── ...
├── checks/                      # integrity checks (see below)
└── archive/                     # completed work, historical records
```

**Member directories**: every member gets their own directory. This is
their workspace within the org. They can put anything there — notes,
scripts, Excel sheets, draft proposals, experimental code. It's theirs
to organize however they want. The point is that their work is visible
to the rest of the organization without requiring them to publish it
anywhere else. Members can also add others as reviewers for PRs from
their directory when they want to share work-in-progress or get
feedback before promoting to the golden copy.

When a member wants to promote something from their personal directory
to the project-wide golden copy, they open a PR moving or copying it
into `docs/`. This is the mechanism for requesting visibility: the PR
itself is a request for the organization to recognize and adopt the
work. Anyone can submit a PR and demo for organizational changes.
Other members review it, and the merge is the organization saying
"yes, this is now part of our shared state."

**The `docs/` directory**: the golden copy. This is the authoritative
state of the project — governance, research agenda, resource records,
everything the organization has collectively agreed on. Nothing gets
into `docs/` without a PR. The PR history in `docs/` is the lab's
institutional memory.

**The `checks/` directory**: automated integrity checks that project
managers create and maintain. These run against the repo (via CI or
on-demand) and surface issues that humans should look at.

### Integrity checks

Integrity checks function similarly to tests in AI-generated code.
When AI agents produce code, tests check that the code does what it
claims. When an organization produces documents, decisions, and
resource allocations, integrity checks do the same thing: they check
for BS and possible issues that should be surfaced to people who care.

This applies to any organization that manages funding — public or
private, academic or corporate, grant-funded or revenue-funded. The
checks are about resource integrity and organizational coherence,
not specific to any particular funding model.

The checks also maintain a history of both automated and human-assisted
audits. Every check run is logged. When a human reviews an issue
surfaced by a check and makes a judgment call, that judgment is
recorded too. Over time this builds an audit trail that shows not
just the current state but how the organization has responded to
issues as they were discovered.

Examples:

- **Consistency checks**: does the resource allocation in
  `docs/resources/` add up? Do the numbers in the budget spreadsheet
  match the grant amounts recorded in governance decisions?

- **Staleness checks**: are there proposals in member directories
  that have been sitting for months with no PR to `docs/`? Are there
  governance documents that reference people who are no longer active?

- **Completeness checks**: does every active grant have an allocation
  record? Does every member have an onboarding document? Does every
  research project referenced in the agenda have a corresponding pm
  instance in the tech tree?

- **Conflict checks**: are there contradictory statements across
  different governance documents? Did two proposals get merged that
  allocate the same funds differently?

- **Attribution checks**: does every work product in `docs/` have
  clear attribution? Are there documents that reference work without
  crediting the member who did it?

These checks can be simple scripts (grep for inconsistencies), Claude-
powered analyses (read the docs and flag things that don't make sense),
or structured validators (parse YAML/CSV and verify invariants).
Project managers create and tune them over time. They're not gates —
they surface issues for human judgment, same as a test suite surfaces
failures for a developer to evaluate.

The checks themselves are in the repo, so anyone can see what's being
checked, propose new checks, or argue that a check is wrong. The
checking infrastructure is as open as everything else.

### Automated recognition

The same infrastructure that detects problems also recognizes
achievements, without the biases that plague human recognition.

In traditional organizations, recognition flows through managers.
This means it's shaped by who is visible to the manager, who is in
their social circle, who shares their background. Nepotism and clan
affiliations distort recognition the same way they distort resource
allocation. The people who get recognized are often the people who
are best at self-promotion, not the people doing the most important
work.

Automated recognition removes this. The data is already there — the
org repo tracks work products, the pm repo tracks PRs and merges,
and the member directories show who did what. Recognition is a read
operation on state that's already being maintained, enabling
visibility into what people are contributing without requiring any
central authority to decide who matters.

Examples of what automated recognition surfaces:

- **Milestone completion**: a plan's entire tech tree reaches merged
  status. The dashboard highlights it — this is a significant
  organizational achievement, not just another PR.

- **First contributions**: a new member's first PR to `docs/` gets
  merged. Visible on the dashboard. This matters because onboarding
  is hard and the organization should notice when someone crosses
  the threshold from observer to contributor.

- **Sustained output**: a member has been consistently contributing
  work products over a period. Not a leaderboard or a score — just
  visibility that steady work is happening, because steady work is
  easy to overlook in favor of flashy launches.

- **Unblocking impact**: a PR merges and unblocks several downstream
  projects in the recursive tech tree. The dashboard shows the
  cascade — the person who did that work enabled a whole layer of
  progress.

- **Funding milestones**: a grant's deliverables are all complete.
  The chain of funding → allocation → work → output is visible
  end-to-end. Useful for the organization's own awareness and for
  demonstrating impact to funders.

- **Cross-project collaboration**: work products that span multiple
  research projects, or a member contributing to a project outside
  their usual area. The organization should see when silos are
  breaking down.

These are generated by the same kinds of scripts and Claude-powered
analyses that power integrity checks. They live in the org repo
alongside the checks, are tunable by PMs, and are as visible and
forkable as everything else. The recognition criteria are not hidden
in someone's head — they're code in the repo that anyone can read,
propose changes to, or argue about.

The dashboards (mobile web, TUI, or whatever interfaces exist) show
these alongside project status. The point is that the organization
notices what its members accomplish without requiring anyone to
self-promote or anyone in management to remember to say something.
The system does the noticing. People decide what to do with it.

Critically, the integration with Claude Code means recognition doesn't
have to target any specific metrics. Fixed metrics get gamed — lines
of code, number of PRs, commit frequency, citation counts. They
reward the metric instead of the work. Because everything in the org
repo and pm repo is readable text and structured data,
anyone can ask Claude to look at what anyone else is doing and assess
their contributions holistically, with context. Claude can read a
member's directory, their PRs, the projects they've touched, the
discussions they've participated in, and explain what that person has
been contributing and where — or whether there are gaps. This is a
fundamentally different thing from a dashboard counting commits. It's
closer to what a thoughtful colleague would say if they'd been paying
attention to everything, which no human can do at organizational scale
but a machine intelligence can.

This is also how Claude becomes the new organizational glue. Instead
of managers spending their time tracking who's doing what, people
can focus on their actual work. Claude handles the noticing, the
connecting, the pattern-recognition across the organization. It
allows people to work at any place and time — there's no need to be
physically present or in the right meeting to be visible. Your work
speaks for itself because a machine intelligence is always reading it.

This also means the organization doesn't have to agree on what metrics
matter. Different people can ask different questions. A PM can ask
"who's been quietly enabling other people's work?" A funder can ask
"what did this grant produce?" A new member can ask "who should I talk
to about X?" The answers come from the same data but through different
lenses, and none of those lenses are baked into the system as the
official way to measure contribution. The data is open. The
interpretation is on-demand. Nothing is gameable because nothing is
fixed.

To be explicit: this is not a tool for bean counting. It will expose
what looks like inefficiency but is actually useful. Technology work
already functions as a kind of baby UBI — people exploring, learning,
building things that don't immediately pay off — and that's not a
problem to be optimized away. The point is visibility into what's
happening, not control over it. The organization sees the work. People
decide what it means.

### The project management repo

Separate from the org repo. Managed by `pm`. Contains `project.yaml`,
`plans/`, and the tech tree. This is where the work planning happens:
what needs to be built, in what order, who's working on what.

The pm repo integrates with the recursive tech tree system described
in `plans/recursive-tech-trees.md`. The lab's top-level tree connects
the organizational layer to all the research projects below it.
Prescriptive mode for lab-wide initiatives (the org decides something
needs to happen and suggests it to a project). Descriptive mode for
observing what projects are doing without directing them.

The org repo and pm repo reference each other but are separate
concerns: the org repo is the record of what the organization is and
has done, the pm repo is the plan for what it's doing next. But it
should be explicit: the pm repo will expose what looks like
inefficiency. It will show when things are blocked, when work is
sitting idle, when plans are stalled. Some of that is real
inefficiency. Some of it is useful exploration. The visibility is
the point — it lets the organization have honest conversations about
what's happening instead of relying on status reports that tell people
what they want to hear.

